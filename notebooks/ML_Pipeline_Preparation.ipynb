{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML Pipeline Preparation.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mai0elkady/Disaster_response_pipelines/blob/main/notebooks/ML_Pipeline_Preparation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvleiwYN7sMa",
        "outputId": "5123f4d4-9eba-463d-ceba-6ddb52133908"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7r5a2SbM7h_q"
      },
      "source": [
        "# ML Pipeline Preparation\n",
        "Follow the instructions below to help you create your ML pipeline.\n",
        "### 1. Import libraries and load data from database.\n",
        "- Import Python libraries\n",
        "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
        "- Define feature and target variables X and Y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKWOJ52A7h_u",
        "outputId": "572177ee-a741-49f1-a17a-339c2a07b140"
      },
      "source": [
        "# import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sqlalchemy import create_engine\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjYwHa1_7h_x",
        "outputId": "47bf38eb-8532-42ab-fe28-54d425c802f1"
      },
      "source": [
        "import sklearn\n",
        "print(sklearn.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.22.2.post1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_WfyJ2r7h_0"
      },
      "source": [
        "# load data from database\n",
        "engine = create_engine('sqlite:////content/drive/MyDrive/UdacityND/project2/data/disaster_data.db')\n",
        "df = pd.read_sql_table('messages_categories',engine)\n",
        "\n",
        "X = df['message'].values\n",
        "Y = df.iloc[:,4:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dui0QssP7h_2",
        "outputId": "08fbde42-03a3-4818-e1b6-9e76d9db1a1e"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(26216,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hp8gVGy17h_3",
        "outputId": "e629f7fb-b3fe-4763-e5d3-ed7d7ee4ed57"
      },
      "source": [
        "Y.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(26216, 35)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgj-kD3A7h_5"
      },
      "source": [
        "### 2. Write a tokenization function to process your text data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tkj_c7UQ7h_7"
      },
      "source": [
        "def tokenize(text):\n",
        "    url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
        "    detected_urls = re.findall(url_regex, text)\n",
        "    for url in detected_urls:\n",
        "        text = text.replace(url, \"urlplaceholder\")\n",
        "    words = word_tokenize(text)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    clean_tokens = []\n",
        "    for tok in words:\n",
        "        if tok not in stopwords.words(\"english\"):\n",
        "            clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
        "            clean_tokens.append(clean_tok)\n",
        "    return clean_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zud62cKA7h_9"
      },
      "source": [
        "### 3. Build a machine learning pipeline\n",
        "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8B88NRL7h_-"
      },
      "source": [
        "# build pipeline\n",
        "pipeline = Pipeline([\n",
        "        ('vect', CountVectorizer(tokenizer=tokenize)),\n",
        "        ('tfidf', TfidfTransformer()),\n",
        "        ('clf', MultiOutputClassifier(RandomForestClassifier(random_state = 0)))\n",
        "])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEX4TxvG7h__",
        "outputId": "89c8dc15-fd86-41a3-dec8-c0d59c9a126c"
      },
      "source": [
        "pipeline.get_params()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'clf': MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True,\n",
              "                                                        ccp_alpha=0.0,\n",
              "                                                        class_weight=None,\n",
              "                                                        criterion='gini',\n",
              "                                                        max_depth=None,\n",
              "                                                        max_features='auto',\n",
              "                                                        max_leaf_nodes=None,\n",
              "                                                        max_samples=None,\n",
              "                                                        min_impurity_decrease=0.0,\n",
              "                                                        min_impurity_split=None,\n",
              "                                                        min_samples_leaf=1,\n",
              "                                                        min_samples_split=2,\n",
              "                                                        min_weight_fraction_leaf=0.0,\n",
              "                                                        n_estimators=100,\n",
              "                                                        n_jobs=None,\n",
              "                                                        oob_score=False,\n",
              "                                                        random_state=None,\n",
              "                                                        verbose=0,\n",
              "                                                        warm_start=False),\n",
              "                       n_jobs=None),\n",
              " 'clf__estimator': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                        criterion='gini', max_depth=None, max_features='auto',\n",
              "                        max_leaf_nodes=None, max_samples=None,\n",
              "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                        min_samples_leaf=1, min_samples_split=2,\n",
              "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
              "                        n_jobs=None, oob_score=False, random_state=None,\n",
              "                        verbose=0, warm_start=False),\n",
              " 'clf__estimator__bootstrap': True,\n",
              " 'clf__estimator__ccp_alpha': 0.0,\n",
              " 'clf__estimator__class_weight': None,\n",
              " 'clf__estimator__criterion': 'gini',\n",
              " 'clf__estimator__max_depth': None,\n",
              " 'clf__estimator__max_features': 'auto',\n",
              " 'clf__estimator__max_leaf_nodes': None,\n",
              " 'clf__estimator__max_samples': None,\n",
              " 'clf__estimator__min_impurity_decrease': 0.0,\n",
              " 'clf__estimator__min_impurity_split': None,\n",
              " 'clf__estimator__min_samples_leaf': 1,\n",
              " 'clf__estimator__min_samples_split': 2,\n",
              " 'clf__estimator__min_weight_fraction_leaf': 0.0,\n",
              " 'clf__estimator__n_estimators': 100,\n",
              " 'clf__estimator__n_jobs': None,\n",
              " 'clf__estimator__oob_score': False,\n",
              " 'clf__estimator__random_state': None,\n",
              " 'clf__estimator__verbose': 0,\n",
              " 'clf__estimator__warm_start': False,\n",
              " 'clf__n_jobs': None,\n",
              " 'memory': None,\n",
              " 'steps': [('vect',\n",
              "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
              "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
              "                   ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
              "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                   tokenizer=<function tokenize at 0x7f298505b9e0>,\n",
              "                   vocabulary=None)),\n",
              "  ('tfidf',\n",
              "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
              "  ('clf',\n",
              "   MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True,\n",
              "                                                          ccp_alpha=0.0,\n",
              "                                                          class_weight=None,\n",
              "                                                          criterion='gini',\n",
              "                                                          max_depth=None,\n",
              "                                                          max_features='auto',\n",
              "                                                          max_leaf_nodes=None,\n",
              "                                                          max_samples=None,\n",
              "                                                          min_impurity_decrease=0.0,\n",
              "                                                          min_impurity_split=None,\n",
              "                                                          min_samples_leaf=1,\n",
              "                                                          min_samples_split=2,\n",
              "                                                          min_weight_fraction_leaf=0.0,\n",
              "                                                          n_estimators=100,\n",
              "                                                          n_jobs=None,\n",
              "                                                          oob_score=False,\n",
              "                                                          random_state=None,\n",
              "                                                          verbose=0,\n",
              "                                                          warm_start=False),\n",
              "                         n_jobs=None))],\n",
              " 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
              " 'tfidf__norm': 'l2',\n",
              " 'tfidf__smooth_idf': True,\n",
              " 'tfidf__sublinear_tf': False,\n",
              " 'tfidf__use_idf': True,\n",
              " 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
              "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
              "                 ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
              "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                 tokenizer=<function tokenize at 0x7f298505b9e0>,\n",
              "                 vocabulary=None),\n",
              " 'vect__analyzer': 'word',\n",
              " 'vect__binary': False,\n",
              " 'vect__decode_error': 'strict',\n",
              " 'vect__dtype': numpy.int64,\n",
              " 'vect__encoding': 'utf-8',\n",
              " 'vect__input': 'content',\n",
              " 'vect__lowercase': True,\n",
              " 'vect__max_df': 1.0,\n",
              " 'vect__max_features': None,\n",
              " 'vect__min_df': 1,\n",
              " 'vect__ngram_range': (1, 1),\n",
              " 'vect__preprocessor': None,\n",
              " 'vect__stop_words': None,\n",
              " 'vect__strip_accents': None,\n",
              " 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              " 'vect__tokenizer': <function __main__.tokenize>,\n",
              " 'vect__vocabulary': None,\n",
              " 'verbose': False}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiw3jOic7h__"
      },
      "source": [
        "### 4. Train pipeline\n",
        "- Split data into train and test sets\n",
        "- Train pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcEF-nRQ7iAA",
        "outputId": "88b40b40-b71d-4cd0-fa1a-ee808215c707"
      },
      "source": [
        "# train classifier\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state = 42)\n",
        "pipeline.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('vect',\n",
              "                 CountVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
              "                                 input='content', lowercase=True, max_df=1.0,\n",
              "                                 max_features=None, min_df=1,\n",
              "                                 ngram_range=(1, 1), preprocessor=None,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=<function tokenize at...\n",
              "                                                                        ccp_alpha=0.0,\n",
              "                                                                        class_weight=None,\n",
              "                                                                        criterion='gini',\n",
              "                                                                        max_depth=None,\n",
              "                                                                        max_features='auto',\n",
              "                                                                        max_leaf_nodes=None,\n",
              "                                                                        max_samples=None,\n",
              "                                                                        min_impurity_decrease=0.0,\n",
              "                                                                        min_impurity_split=None,\n",
              "                                                                        min_samples_leaf=1,\n",
              "                                                                        min_samples_split=2,\n",
              "                                                                        min_weight_fraction_leaf=0.0,\n",
              "                                                                        n_estimators=100,\n",
              "                                                                        n_jobs=None,\n",
              "                                                                        oob_score=False,\n",
              "                                                                        random_state=None,\n",
              "                                                                        verbose=0,\n",
              "                                                                        warm_start=False),\n",
              "                                       n_jobs=None))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMuty_2O7iAB"
      },
      "source": [
        "### 5. Test your model\n",
        "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSkUZYPY7iAC"
      },
      "source": [
        "# predict on test data\n",
        "y_pred = pipeline.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNdfqXX7_eki",
        "outputId": "0dd46b62-1026-4010-c227-299a5f094deb"
      },
      "source": [
        "print(y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 0 0 ... 0 0 0]\n",
            " [1 0 0 ... 0 0 0]\n",
            " [1 0 0 ... 0 0 0]\n",
            " ...\n",
            " [1 0 0 ... 0 0 0]\n",
            " [1 0 0 ... 0 0 0]\n",
            " [1 0 0 ... 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JhTINwN7iAD",
        "outputId": "7da88dfc-4f08-48a1-c445-b344ccdb5c43"
      },
      "source": [
        "print(classification_report(y_test, y_pred,target_names=y_test.columns.values))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                        precision    recall  f1-score   support\n",
            "\n",
            "               related       0.82      0.95      0.88      4944\n",
            "               request       0.84      0.47      0.60      1111\n",
            "                 offer       0.00      0.00      0.00        33\n",
            "           aid_related       0.76      0.68      0.72      2670\n",
            "          medical_help       0.72      0.10      0.18       535\n",
            "      medical_products       0.85      0.06      0.12       344\n",
            "     search_and_rescue       0.63      0.08      0.13       159\n",
            "              security       0.25      0.01      0.02       116\n",
            "              military       0.55      0.06      0.11       200\n",
            "                 water       0.88      0.27      0.41       418\n",
            "                  food       0.85      0.54      0.66       745\n",
            "               shelter       0.84      0.34      0.48       581\n",
            "              clothing       0.71      0.05      0.10        98\n",
            "                 money       0.80      0.06      0.11       133\n",
            "        missing_people       0.50      0.01      0.03        73\n",
            "              refugees       0.33      0.02      0.04       215\n",
            "                 death       0.85      0.14      0.24       297\n",
            "             other_aid       0.65      0.03      0.06       864\n",
            "infrastructure_related       0.00      0.00      0.00       411\n",
            "             transport       0.76      0.07      0.13       303\n",
            "             buildings       0.82      0.10      0.18       323\n",
            "           electricity       1.00      0.01      0.03       147\n",
            "                 tools       0.00      0.00      0.00        43\n",
            "             hospitals       0.00      0.00      0.00        56\n",
            "                 shops       0.00      0.00      0.00        24\n",
            "           aid_centers       0.00      0.00      0.00        81\n",
            "  other_infrastructure       0.00      0.00      0.00       283\n",
            "       weather_related       0.85      0.68      0.75      1773\n",
            "                floods       0.88      0.45      0.59       519\n",
            "                 storm       0.75      0.53      0.62       605\n",
            "                  fire       0.00      0.00      0.00        66\n",
            "            earthquake       0.89      0.76      0.82       590\n",
            "                  cold       0.89      0.06      0.11       141\n",
            "         other_weather       0.75      0.02      0.03       335\n",
            "         direct_report       0.80      0.33      0.47      1272\n",
            "\n",
            "             micro avg       0.81      0.52      0.63     20508\n",
            "             macro avg       0.58      0.20      0.25     20508\n",
            "          weighted avg       0.75      0.52      0.56     20508\n",
            "           samples avg       0.66      0.47      0.50     20508\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pubXdZk7iAD",
        "outputId": "81a999bb-5d59-4f9b-abec-33f5b0be6bc9"
      },
      "source": [
        "precision_recall_fscore_support(y_test, y_pred, average='micro')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.8099908759124088, 0.5194558221181977, 0.6329768270944741, None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pW9N5lkO7iAD",
        "outputId": "23294e6e-6f53-408b-e866-6c2bd9c42ddd"
      },
      "source": [
        "def eval_results( y_test, y_pred):\n",
        "    print(classification_report(y_test, y_pred,target_names=y_test.columns.values))\n",
        "    result = precision_recall_fscore_support(y_test, y_pred, average='micro')\n",
        "    print(result)\n",
        "        \n",
        "eval_results(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                        precision    recall  f1-score   support\n",
            "\n",
            "               related       0.82      0.95      0.88      4944\n",
            "               request       0.84      0.47      0.60      1111\n",
            "                 offer       0.00      0.00      0.00        33\n",
            "           aid_related       0.76      0.68      0.72      2670\n",
            "          medical_help       0.72      0.10      0.18       535\n",
            "      medical_products       0.85      0.06      0.12       344\n",
            "     search_and_rescue       0.63      0.08      0.13       159\n",
            "              security       0.25      0.01      0.02       116\n",
            "              military       0.55      0.06      0.11       200\n",
            "                 water       0.88      0.27      0.41       418\n",
            "                  food       0.85      0.54      0.66       745\n",
            "               shelter       0.84      0.34      0.48       581\n",
            "              clothing       0.71      0.05      0.10        98\n",
            "                 money       0.80      0.06      0.11       133\n",
            "        missing_people       0.50      0.01      0.03        73\n",
            "              refugees       0.33      0.02      0.04       215\n",
            "                 death       0.85      0.14      0.24       297\n",
            "             other_aid       0.65      0.03      0.06       864\n",
            "infrastructure_related       0.00      0.00      0.00       411\n",
            "             transport       0.76      0.07      0.13       303\n",
            "             buildings       0.82      0.10      0.18       323\n",
            "           electricity       1.00      0.01      0.03       147\n",
            "                 tools       0.00      0.00      0.00        43\n",
            "             hospitals       0.00      0.00      0.00        56\n",
            "                 shops       0.00      0.00      0.00        24\n",
            "           aid_centers       0.00      0.00      0.00        81\n",
            "  other_infrastructure       0.00      0.00      0.00       283\n",
            "       weather_related       0.85      0.68      0.75      1773\n",
            "                floods       0.88      0.45      0.59       519\n",
            "                 storm       0.75      0.53      0.62       605\n",
            "                  fire       0.00      0.00      0.00        66\n",
            "            earthquake       0.89      0.76      0.82       590\n",
            "                  cold       0.89      0.06      0.11       141\n",
            "         other_weather       0.75      0.02      0.03       335\n",
            "         direct_report       0.80      0.33      0.47      1272\n",
            "\n",
            "             micro avg       0.81      0.52      0.63     20508\n",
            "             macro avg       0.58      0.20      0.25     20508\n",
            "          weighted avg       0.75      0.52      0.56     20508\n",
            "           samples avg       0.66      0.47      0.50     20508\n",
            "\n",
            "(0.8099908759124088, 0.5194558221181977, 0.6329768270944741, None)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvzO5aZ07iAF"
      },
      "source": [
        "### 6. Improve your model\n",
        "Use grid search to find better parameters. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQerteHR7iAG"
      },
      "source": [
        "parameters = {'tfidf__use_idf':[True, False],\n",
        "              'clf__estimator__criterion': ['gini','entropy'],\n",
        "              'clf__estimator__n_estimators': [5, 10]}\n",
        "\n",
        "cv = GridSearchCV(pipeline, param_grid=parameters, verbose = 2, scoring='f1_micro', cv=3) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-2jWvSK7iAH"
      },
      "source": [
        "### 7. Test your model\n",
        "Show the accuracy, precision, and recall of the tuned model.  \n",
        "\n",
        "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tO9wzLxJ7iAI",
        "outputId": "5393b78d-4021-4831-8ee0-2b0c02aece7e"
      },
      "source": [
        "cv.fit(X_train,y_train)\n",
        "y_pred_new = cv.predict(X_test)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
            "[CV] clf__estimator__criterion=gini, clf__estimator__n_estimators=5, tfidf__use_idf=True \n",
            "[CV]  clf__estimator__criterion=gini, clf__estimator__n_estimators=5, tfidf__use_idf=True, total= 1.5min\n",
            "[CV] clf__estimator__criterion=gini, clf__estimator__n_estimators=5, tfidf__use_idf=True \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.5min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  clf__estimator__criterion=gini, clf__estimator__n_estimators=5, tfidf__use_idf=True, total= 1.4min\n",
            "[CV] clf__estimator__criterion=gini, clf__estimator__n_estimators=5, tfidf__use_idf=True \n",
            "[CV]  clf__estimator__criterion=gini, clf__estimator__n_estimators=5, tfidf__use_idf=True, total= 1.4min\n",
            "[CV] clf__estimator__criterion=gini, clf__estimator__n_estimators=5, tfidf__use_idf=False \n",
            "[CV]  clf__estimator__criterion=gini, clf__estimator__n_estimators=5, tfidf__use_idf=False, total= 1.4min\n",
            "[CV] clf__estimator__criterion=gini, clf__estimator__n_estimators=5, tfidf__use_idf=False \n",
            "[CV]  clf__estimator__criterion=gini, clf__estimator__n_estimators=5, tfidf__use_idf=False, total= 1.4min\n",
            "[CV] clf__estimator__criterion=gini, clf__estimator__n_estimators=5, tfidf__use_idf=False \n",
            "[CV]  clf__estimator__criterion=gini, clf__estimator__n_estimators=5, tfidf__use_idf=False, total= 1.4min\n",
            "[CV] clf__estimator__criterion=gini, clf__estimator__n_estimators=10, tfidf__use_idf=True \n",
            "[CV]  clf__estimator__criterion=gini, clf__estimator__n_estimators=10, tfidf__use_idf=True, total= 1.7min\n",
            "[CV] clf__estimator__criterion=gini, clf__estimator__n_estimators=10, tfidf__use_idf=True \n",
            "[CV]  clf__estimator__criterion=gini, clf__estimator__n_estimators=10, tfidf__use_idf=True, total= 1.7min\n",
            "[CV] clf__estimator__criterion=gini, clf__estimator__n_estimators=10, tfidf__use_idf=True \n",
            "[CV]  clf__estimator__criterion=gini, clf__estimator__n_estimators=10, tfidf__use_idf=True, total= 1.7min\n",
            "[CV] clf__estimator__criterion=gini, clf__estimator__n_estimators=10, tfidf__use_idf=False \n",
            "[CV]  clf__estimator__criterion=gini, clf__estimator__n_estimators=10, tfidf__use_idf=False, total= 1.7min\n",
            "[CV] clf__estimator__criterion=gini, clf__estimator__n_estimators=10, tfidf__use_idf=False \n",
            "[CV]  clf__estimator__criterion=gini, clf__estimator__n_estimators=10, tfidf__use_idf=False, total= 1.7min\n",
            "[CV] clf__estimator__criterion=gini, clf__estimator__n_estimators=10, tfidf__use_idf=False \n",
            "[CV]  clf__estimator__criterion=gini, clf__estimator__n_estimators=10, tfidf__use_idf=False, total= 1.7min\n",
            "[CV] clf__estimator__criterion=entropy, clf__estimator__n_estimators=5, tfidf__use_idf=True \n",
            "[CV]  clf__estimator__criterion=entropy, clf__estimator__n_estimators=5, tfidf__use_idf=True, total= 1.5min\n",
            "[CV] clf__estimator__criterion=entropy, clf__estimator__n_estimators=5, tfidf__use_idf=True \n",
            "[CV]  clf__estimator__criterion=entropy, clf__estimator__n_estimators=5, tfidf__use_idf=True, total= 1.5min\n",
            "[CV] clf__estimator__criterion=entropy, clf__estimator__n_estimators=5, tfidf__use_idf=True \n",
            "[CV]  clf__estimator__criterion=entropy, clf__estimator__n_estimators=5, tfidf__use_idf=True, total= 1.5min\n",
            "[CV] clf__estimator__criterion=entropy, clf__estimator__n_estimators=5, tfidf__use_idf=False \n",
            "[CV]  clf__estimator__criterion=entropy, clf__estimator__n_estimators=5, tfidf__use_idf=False, total= 1.4min\n",
            "[CV] clf__estimator__criterion=entropy, clf__estimator__n_estimators=5, tfidf__use_idf=False \n",
            "[CV]  clf__estimator__criterion=entropy, clf__estimator__n_estimators=5, tfidf__use_idf=False, total= 1.5min\n",
            "[CV] clf__estimator__criterion=entropy, clf__estimator__n_estimators=5, tfidf__use_idf=False \n",
            "[CV]  clf__estimator__criterion=entropy, clf__estimator__n_estimators=5, tfidf__use_idf=False, total= 1.5min\n",
            "[CV] clf__estimator__criterion=entropy, clf__estimator__n_estimators=10, tfidf__use_idf=True \n",
            "[CV]  clf__estimator__criterion=entropy, clf__estimator__n_estimators=10, tfidf__use_idf=True, total= 1.7min\n",
            "[CV] clf__estimator__criterion=entropy, clf__estimator__n_estimators=10, tfidf__use_idf=True \n",
            "[CV]  clf__estimator__criterion=entropy, clf__estimator__n_estimators=10, tfidf__use_idf=True, total= 1.7min\n",
            "[CV] clf__estimator__criterion=entropy, clf__estimator__n_estimators=10, tfidf__use_idf=True \n",
            "[CV]  clf__estimator__criterion=entropy, clf__estimator__n_estimators=10, tfidf__use_idf=True, total= 1.7min\n",
            "[CV] clf__estimator__criterion=entropy, clf__estimator__n_estimators=10, tfidf__use_idf=False \n",
            "[CV]  clf__estimator__criterion=entropy, clf__estimator__n_estimators=10, tfidf__use_idf=False, total= 1.7min\n",
            "[CV] clf__estimator__criterion=entropy, clf__estimator__n_estimators=10, tfidf__use_idf=False \n",
            "[CV]  clf__estimator__criterion=entropy, clf__estimator__n_estimators=10, tfidf__use_idf=False, total= 1.7min\n",
            "[CV] clf__estimator__criterion=entropy, clf__estimator__n_estimators=10, tfidf__use_idf=False \n",
            "[CV]  clf__estimator__criterion=entropy, clf__estimator__n_estimators=10, tfidf__use_idf=False, total= 1.7min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  24 out of  24 | elapsed: 37.6min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-bce0edcd1a36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my_pred_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0meval_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: eval_results() takes 2 positional arguments but 3 were given"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dosIqvrv7NN-",
        "outputId": "52dbd306-f9e3-453e-f2d5-5603ecee7870"
      },
      "source": [
        "eval_results( y_test, y_pred_new)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                        precision    recall  f1-score   support\n",
            "\n",
            "               related       0.82      0.91      0.86      4944\n",
            "               request       0.68      0.44      0.53      1111\n",
            "                 offer       1.00      0.03      0.06        33\n",
            "           aid_related       0.68      0.64      0.66      2670\n",
            "          medical_help       0.58      0.18      0.27       535\n",
            "      medical_products       0.55      0.12      0.20       344\n",
            "     search_and_rescue       0.23      0.04      0.07       159\n",
            "              security       0.08      0.01      0.02       116\n",
            "              military       0.43      0.10      0.16       200\n",
            "                 water       0.77      0.48      0.59       418\n",
            "                  food       0.76      0.41      0.53       745\n",
            "               shelter       0.72      0.31      0.43       581\n",
            "              clothing       0.68      0.29      0.40        98\n",
            "                 money       0.61      0.08      0.15       133\n",
            "        missing_people       0.29      0.03      0.05        73\n",
            "              refugees       0.47      0.07      0.13       215\n",
            "                 death       0.76      0.27      0.39       297\n",
            "             other_aid       0.37      0.08      0.13       864\n",
            "infrastructure_related       0.15      0.02      0.03       411\n",
            "             transport       0.42      0.11      0.17       303\n",
            "             buildings       0.58      0.14      0.23       323\n",
            "           electricity       0.67      0.11      0.19       147\n",
            "                 tools       0.00      0.00      0.00        43\n",
            "             hospitals       0.00      0.00      0.00        56\n",
            "                 shops       0.00      0.00      0.00        24\n",
            "           aid_centers       0.00      0.00      0.00        81\n",
            "  other_infrastructure       0.15      0.01      0.03       283\n",
            "       weather_related       0.75      0.62      0.68      1773\n",
            "                floods       0.78      0.42      0.55       519\n",
            "                 storm       0.69      0.46      0.55       605\n",
            "                  fire       1.00      0.05      0.09        66\n",
            "            earthquake       0.84      0.62      0.71       590\n",
            "                  cold       0.52      0.08      0.14       141\n",
            "         other_weather       0.40      0.07      0.12       335\n",
            "         direct_report       0.59      0.36      0.45      1272\n",
            "\n",
            "             micro avg       0.73      0.50      0.60     20508\n",
            "             macro avg       0.51      0.22      0.27     20508\n",
            "          weighted avg       0.66      0.50      0.54     20508\n",
            "           samples avg       0.60      0.45      0.47     20508\n",
            "\n",
            "(0.7313380032547937, 0.5039984396333138, 0.596749516468924, None)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btOI43nNX_ol",
        "outputId": "0901585d-ecc0-438c-ff4c-00ebc871fda3"
      },
      "source": [
        "print(cv.best_params_)\n",
        "best_model = cv.best_estimator_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'clf__estimator__criterion': 'gini', 'clf__estimator__n_estimators': 5, 'tfidf__use_idf': True}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BONcYHTN7iAJ"
      },
      "source": [
        "### 8. Try improving your model further. Here are a few ideas:\n",
        "* try other machine learning algorithms\n",
        "* add other features besides the TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qARHrS4j7iAJ",
        "outputId": "c765ef46-b1c9-4981-bbf3-fe818f8b41a1"
      },
      "source": [
        "# build pipeline\n",
        "pipeline_logreg = Pipeline([\n",
        "        ('vect', CountVectorizer(tokenizer=tokenize)),\n",
        "        ('tfidf', TfidfTransformer()),\n",
        "        ('clf', MultiOutputClassifier(LogisticRegression(random_state=0)))\n",
        "])\n",
        "\n",
        "pipeline_logreg.get_params()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'clf': MultiOutputClassifier(estimator=LogisticRegression(C=1.0, class_weight=None,\n",
              "                                                    dual=False,\n",
              "                                                    fit_intercept=True,\n",
              "                                                    intercept_scaling=1,\n",
              "                                                    l1_ratio=None, max_iter=100,\n",
              "                                                    multi_class='auto',\n",
              "                                                    n_jobs=None, penalty='l2',\n",
              "                                                    random_state=0,\n",
              "                                                    solver='lbfgs', tol=0.0001,\n",
              "                                                    verbose=0,\n",
              "                                                    warm_start=False),\n",
              "                       n_jobs=None),\n",
              " 'clf__estimator': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                    random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                    warm_start=False),\n",
              " 'clf__estimator__C': 1.0,\n",
              " 'clf__estimator__class_weight': None,\n",
              " 'clf__estimator__dual': False,\n",
              " 'clf__estimator__fit_intercept': True,\n",
              " 'clf__estimator__intercept_scaling': 1,\n",
              " 'clf__estimator__l1_ratio': None,\n",
              " 'clf__estimator__max_iter': 100,\n",
              " 'clf__estimator__multi_class': 'auto',\n",
              " 'clf__estimator__n_jobs': None,\n",
              " 'clf__estimator__penalty': 'l2',\n",
              " 'clf__estimator__random_state': 0,\n",
              " 'clf__estimator__solver': 'lbfgs',\n",
              " 'clf__estimator__tol': 0.0001,\n",
              " 'clf__estimator__verbose': 0,\n",
              " 'clf__estimator__warm_start': False,\n",
              " 'clf__n_jobs': None,\n",
              " 'memory': None,\n",
              " 'steps': [('vect',\n",
              "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
              "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
              "                   ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
              "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                   tokenizer=<function tokenize at 0x7f298505b9e0>,\n",
              "                   vocabulary=None)),\n",
              "  ('tfidf',\n",
              "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
              "  ('clf',\n",
              "   MultiOutputClassifier(estimator=LogisticRegression(C=1.0, class_weight=None,\n",
              "                                                      dual=False,\n",
              "                                                      fit_intercept=True,\n",
              "                                                      intercept_scaling=1,\n",
              "                                                      l1_ratio=None, max_iter=100,\n",
              "                                                      multi_class='auto',\n",
              "                                                      n_jobs=None, penalty='l2',\n",
              "                                                      random_state=0,\n",
              "                                                      solver='lbfgs', tol=0.0001,\n",
              "                                                      verbose=0,\n",
              "                                                      warm_start=False),\n",
              "                         n_jobs=None))],\n",
              " 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
              " 'tfidf__norm': 'l2',\n",
              " 'tfidf__smooth_idf': True,\n",
              " 'tfidf__sublinear_tf': False,\n",
              " 'tfidf__use_idf': True,\n",
              " 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
              "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
              "                 ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
              "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                 tokenizer=<function tokenize at 0x7f298505b9e0>,\n",
              "                 vocabulary=None),\n",
              " 'vect__analyzer': 'word',\n",
              " 'vect__binary': False,\n",
              " 'vect__decode_error': 'strict',\n",
              " 'vect__dtype': numpy.int64,\n",
              " 'vect__encoding': 'utf-8',\n",
              " 'vect__input': 'content',\n",
              " 'vect__lowercase': True,\n",
              " 'vect__max_df': 1.0,\n",
              " 'vect__max_features': None,\n",
              " 'vect__min_df': 1,\n",
              " 'vect__ngram_range': (1, 1),\n",
              " 'vect__preprocessor': None,\n",
              " 'vect__stop_words': None,\n",
              " 'vect__strip_accents': None,\n",
              " 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              " 'vect__tokenizer': <function __main__.tokenize>,\n",
              " 'vect__vocabulary': None,\n",
              " 'verbose': False}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-G0EXgHiWVPa",
        "outputId": "c415187d-2ffe-42cb-ae7a-195f70ea26f3"
      },
      "source": [
        "pipeline_logreg.fit(X_train,y_train)\n",
        "y_pred = pipeline_logreg.predict(X_test)\n",
        "eval_results(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                        precision    recall  f1-score   support\n",
            "\n",
            "               related       0.83      0.95      0.89      4944\n",
            "               request       0.83      0.53      0.65      1111\n",
            "                 offer       0.00      0.00      0.00        33\n",
            "           aid_related       0.76      0.67      0.71      2670\n",
            "          medical_help       0.67      0.16      0.26       535\n",
            "      medical_products       0.82      0.18      0.30       344\n",
            "     search_and_rescue       1.00      0.04      0.08       159\n",
            "              security       0.00      0.00      0.00       116\n",
            "              military       0.57      0.08      0.14       200\n",
            "                 water       0.80      0.47      0.59       418\n",
            "                  food       0.87      0.59      0.70       745\n",
            "               shelter       0.83      0.48      0.60       581\n",
            "              clothing       0.88      0.14      0.25        98\n",
            "                 money       0.63      0.09      0.16       133\n",
            "        missing_people       1.00      0.01      0.03        73\n",
            "              refugees       0.61      0.07      0.12       215\n",
            "                 death       0.93      0.25      0.39       297\n",
            "             other_aid       0.55      0.11      0.19       864\n",
            "infrastructure_related       0.43      0.02      0.04       411\n",
            "             transport       0.79      0.08      0.14       303\n",
            "             buildings       0.92      0.20      0.33       323\n",
            "           electricity       0.79      0.07      0.14       147\n",
            "                 tools       0.00      0.00      0.00        43\n",
            "             hospitals       0.00      0.00      0.00        56\n",
            "                 shops       0.00      0.00      0.00        24\n",
            "           aid_centers       0.00      0.00      0.00        81\n",
            "  other_infrastructure       0.20      0.00      0.01       283\n",
            "       weather_related       0.85      0.65      0.74      1773\n",
            "                floods       0.91      0.42      0.58       519\n",
            "                 storm       0.74      0.45      0.56       605\n",
            "                  fire       0.50      0.02      0.03        66\n",
            "            earthquake       0.90      0.65      0.75       590\n",
            "                  cold       0.76      0.09      0.16       141\n",
            "         other_weather       0.47      0.03      0.05       335\n",
            "         direct_report       0.74      0.43      0.54      1272\n",
            "\n",
            "             micro avg       0.81      0.54      0.65     20508\n",
            "             macro avg       0.62      0.23      0.29     20508\n",
            "          weighted avg       0.76      0.54      0.59     20508\n",
            "           samples avg       0.65      0.48      0.50     20508\n",
            "\n",
            "(0.8107297079704311, 0.5401306807099668, 0.6483275291914196, None)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_AxWrahWxPy"
      },
      "source": [
        "parameters = {'tfidf__use_idf':[True, False],\n",
        "              'clf__estimator__C': [0.001,10,100],\n",
        "              }\n",
        "\n",
        "cv = GridSearchCV(pipeline_logreg, param_grid=parameters, verbose = 2, scoring='f1_micro') \n",
        "cv.fit(X_train,y_train)\n",
        "y_pred_new = cv.predict(X_test)\n",
        "eval_results(y_test, y_pred_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NuC9iIqb9bL",
        "outputId": "490fee07-3a4a-4a36-ff8d-9f05d2286f50"
      },
      "source": [
        "print(cv.best_params_)\n",
        "best_model = cv.best_estimator_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'clf__estimator__C': 10, 'tfidf__use_idf': False}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_v2k6SHhPPu",
        "outputId": "9bf0fd65-9881-4b70-c277-9bf50f8fbe8e"
      },
      "source": [
        "# build pipeline\n",
        "pipeline_SVM = Pipeline([\n",
        "        ('vect', CountVectorizer(tokenizer=tokenize)),\n",
        "        ('tfidf', TfidfTransformer()),\n",
        "        ('clf', MultiOutputClassifier(SVC(random_state=0)))\n",
        "])\n",
        "\n",
        "pipeline_SVM.get_params()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'clf': MultiOutputClassifier(estimator=SVC(C=1.0, break_ties=False, cache_size=200,\n",
              "                                     class_weight=None, coef0=0.0,\n",
              "                                     decision_function_shape='ovr', degree=3,\n",
              "                                     gamma='scale', kernel='rbf', max_iter=-1,\n",
              "                                     probability=False, random_state=0,\n",
              "                                     shrinking=True, tol=0.001, verbose=False),\n",
              "                       n_jobs=None),\n",
              " 'clf__estimator': SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
              "     decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
              "     max_iter=-1, probability=False, random_state=0, shrinking=True, tol=0.001,\n",
              "     verbose=False),\n",
              " 'clf__estimator__C': 1.0,\n",
              " 'clf__estimator__break_ties': False,\n",
              " 'clf__estimator__cache_size': 200,\n",
              " 'clf__estimator__class_weight': None,\n",
              " 'clf__estimator__coef0': 0.0,\n",
              " 'clf__estimator__decision_function_shape': 'ovr',\n",
              " 'clf__estimator__degree': 3,\n",
              " 'clf__estimator__gamma': 'scale',\n",
              " 'clf__estimator__kernel': 'rbf',\n",
              " 'clf__estimator__max_iter': -1,\n",
              " 'clf__estimator__probability': False,\n",
              " 'clf__estimator__random_state': 0,\n",
              " 'clf__estimator__shrinking': True,\n",
              " 'clf__estimator__tol': 0.001,\n",
              " 'clf__estimator__verbose': False,\n",
              " 'clf__n_jobs': None,\n",
              " 'memory': None,\n",
              " 'steps': [('vect',\n",
              "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
              "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
              "                   ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
              "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                   tokenizer=<function tokenize at 0x7f298505b9e0>,\n",
              "                   vocabulary=None)),\n",
              "  ('tfidf',\n",
              "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
              "  ('clf',\n",
              "   MultiOutputClassifier(estimator=SVC(C=1.0, break_ties=False, cache_size=200,\n",
              "                                       class_weight=None, coef0=0.0,\n",
              "                                       decision_function_shape='ovr', degree=3,\n",
              "                                       gamma='scale', kernel='rbf', max_iter=-1,\n",
              "                                       probability=False, random_state=0,\n",
              "                                       shrinking=True, tol=0.001, verbose=False),\n",
              "                         n_jobs=None))],\n",
              " 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
              " 'tfidf__norm': 'l2',\n",
              " 'tfidf__smooth_idf': True,\n",
              " 'tfidf__sublinear_tf': False,\n",
              " 'tfidf__use_idf': True,\n",
              " 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
              "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
              "                 ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
              "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                 tokenizer=<function tokenize at 0x7f298505b9e0>,\n",
              "                 vocabulary=None),\n",
              " 'vect__analyzer': 'word',\n",
              " 'vect__binary': False,\n",
              " 'vect__decode_error': 'strict',\n",
              " 'vect__dtype': numpy.int64,\n",
              " 'vect__encoding': 'utf-8',\n",
              " 'vect__input': 'content',\n",
              " 'vect__lowercase': True,\n",
              " 'vect__max_df': 1.0,\n",
              " 'vect__max_features': None,\n",
              " 'vect__min_df': 1,\n",
              " 'vect__ngram_range': (1, 1),\n",
              " 'vect__preprocessor': None,\n",
              " 'vect__stop_words': None,\n",
              " 'vect__strip_accents': None,\n",
              " 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              " 'vect__tokenizer': <function __main__.tokenize>,\n",
              " 'vect__vocabulary': None,\n",
              " 'verbose': False}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "6moUV0HthsfQ",
        "outputId": "35ed9570-db8a-46e7-ee86-6d557ac00d00"
      },
      "source": [
        "parameters = {'tfidf__use_idf':[True, False],\n",
        "              'clf__estimator__C': [0.001,1,10,100],\n",
        "              }\n",
        "\n",
        "cv = GridSearchCV(pipeline_SVM, param_grid=parameters, verbose = 2, scoring='f1_micro') \n",
        "cv.fit(X_train,y_train)\n",
        "y_pred_new = cv.predict(X_test)\n",
        "eval_results(y_test, y_pred_new)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
            "[CV] clf__estimator__C=0.001, tfidf__use_idf=True ....................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV] ..... clf__estimator__C=0.001, tfidf__use_idf=True, total= 7.0min\n",
            "[CV] clf__estimator__C=0.001, tfidf__use_idf=True ....................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  7.0min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV] ..... clf__estimator__C=0.001, tfidf__use_idf=True, total= 7.5min\n",
            "[CV] clf__estimator__C=0.001, tfidf__use_idf=True ....................\n",
            "[CV] ..... clf__estimator__C=0.001, tfidf__use_idf=True, total= 7.6min\n",
            "[CV] clf__estimator__C=0.001, tfidf__use_idf=True ....................\n",
            "[CV] ..... clf__estimator__C=0.001, tfidf__use_idf=True, total= 8.0min\n",
            "[CV] clf__estimator__C=0.001, tfidf__use_idf=True ....................\n",
            "[CV] ..... clf__estimator__C=0.001, tfidf__use_idf=True, total= 7.9min\n",
            "[CV] clf__estimator__C=0.001, tfidf__use_idf=False ...................\n",
            "[CV] .... clf__estimator__C=0.001, tfidf__use_idf=False, total= 8.2min\n",
            "[CV] clf__estimator__C=0.001, tfidf__use_idf=False ...................\n",
            "[CV] .... clf__estimator__C=0.001, tfidf__use_idf=False, total= 8.3min\n",
            "[CV] clf__estimator__C=0.001, tfidf__use_idf=False ...................\n",
            "[CV] .... clf__estimator__C=0.001, tfidf__use_idf=False, total= 8.3min\n",
            "[CV] clf__estimator__C=0.001, tfidf__use_idf=False ...................\n",
            "[CV] .... clf__estimator__C=0.001, tfidf__use_idf=False, total= 8.3min\n",
            "[CV] clf__estimator__C=0.001, tfidf__use_idf=False ...................\n",
            "[CV] .... clf__estimator__C=0.001, tfidf__use_idf=False, total= 8.3min\n",
            "[CV] clf__estimator__C=1, tfidf__use_idf=True ........................\n",
            "[CV] ......... clf__estimator__C=1, tfidf__use_idf=True, total=37.7min\n",
            "[CV] clf__estimator__C=1, tfidf__use_idf=True ........................\n",
            "[CV] ......... clf__estimator__C=1, tfidf__use_idf=True, total=38.2min\n",
            "[CV] clf__estimator__C=1, tfidf__use_idf=True ........................\n",
            "[CV] ......... clf__estimator__C=1, tfidf__use_idf=True, total=37.6min\n",
            "[CV] clf__estimator__C=1, tfidf__use_idf=True ........................\n",
            "[CV] ......... clf__estimator__C=1, tfidf__use_idf=True, total=35.7min\n",
            "[CV] clf__estimator__C=1, tfidf__use_idf=True ........................\n",
            "[CV] ......... clf__estimator__C=1, tfidf__use_idf=True, total=35.5min\n",
            "[CV] clf__estimator__C=1, tfidf__use_idf=False .......................\n",
            "[CV] ........ clf__estimator__C=1, tfidf__use_idf=False, total=26.2min\n",
            "[CV] clf__estimator__C=1, tfidf__use_idf=False .......................\n",
            "[CV] ........ clf__estimator__C=1, tfidf__use_idf=False, total=26.9min\n",
            "[CV] clf__estimator__C=1, tfidf__use_idf=False .......................\n",
            "[CV] ........ clf__estimator__C=1, tfidf__use_idf=False, total=26.7min\n",
            "[CV] clf__estimator__C=1, tfidf__use_idf=False .......................\n",
            "[CV] ........ clf__estimator__C=1, tfidf__use_idf=False, total=26.1min\n",
            "[CV] clf__estimator__C=1, tfidf__use_idf=False .......................\n",
            "[CV] ........ clf__estimator__C=1, tfidf__use_idf=False, total=26.7min\n",
            "[CV] clf__estimator__C=10, tfidf__use_idf=True .......................\n",
            "[CV] ........ clf__estimator__C=10, tfidf__use_idf=True, total=50.1min\n",
            "[CV] clf__estimator__C=10, tfidf__use_idf=True .......................\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pFTdwgW7iAK"
      },
      "source": [
        "### 9. Export your model as a pickle file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKHfJF0u7iAK"
      },
      "source": [
        "import pickle\n",
        "pickle.dump(best_model, open(\"classifier\", 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyZWpKW27iAK"
      },
      "source": [
        "### 10. Use this notebook to complete `train.py`\n",
        "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgKYoGEc7iAK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}